### One-command install (works anywhere Nanobot runs â€” macOS, Linux, Docker, brew-installed CLI)

```bash
curl -fsSL https://raw.githubusercontent.com/profbernardoj/nanobot-everclaw/main/setup.sh | bash
```

Then:

```bash
# Use the pre-created example config (or copy to your own nanobot.yaml)
OPENAI_API_KEY=sk-morpheus \
OPENAI_BASE_URL=http://127.0.0.1:8083/v1 \
nanobot run ~/nanobot-morpheus.yaml
```

Your Nanobot agents instantly switch to **decentralized Morpheus inference** (GLM-5 heavy default, GLM-4.7-flash fast, Kimi K2.5, Qwen3-235b, etc.) via the local OpenAI-compatible proxy on `http://127.0.0.1:8083/v1`.

No code changes. No rebuilds. No API bills. Stake MOR once â†’ unlimited use forever. Nanobotâ€™s lightweight Go + Svelte MCP host stays untouched.

### Whatâ€™s in the repo (ready right now)

- `README.md` â€” full docs, MCP-specific tips, example agent configs, staking, troubleshooting
- `setup.sh` â€” installs EverClaw proxy + guardian into `~/.everclaw`, starts it as service (launchd/systemd), creates `~/nanobot-morpheus.yaml` example + helper wrapper
- `nanobot-morpheus.yaml` â€” ready-to-use single-file config with GLM-5 agent + MCP example
- `examples/` â€” directory-style config with multiple Morpheus models in agent frontmatter
- `tools-src/morpheus-status/` â€” optional Go tool (compile & add as MCP server if desired)

### What setup.sh does automatically

1. Clones & installs the battle-tested EverClaw proxy + guardian (Node.js sidecar, ~80 MB total)
2. Starts proxy on boot (systemd user service or launchd)
3. Creates `~/nanobot-morpheus.yaml` example:

```yaml
agents:
  main:
    name: Morpheus Agent
    model: glm-5                    # or glm-4.7-flash / kimi-k2.5 / qwen3-235b
    temperature: 0.7
    mcpServers: []                  # add your MCP servers here

# Your existing MCP servers go here...
```

4. Prints one-liner to run with correct env vars
5. Adds optional alias `nanobot-morpheus` to `~/.bashrc` / `~/.zshrc` (just `nanobot-morpheus` to launch)

### Staking (unlimited use â€” same as all EverClaw forks)

```bash
cd ~/.everclaw
node scripts/everclaw-wallet.mjs setup
node scripts/everclaw-wallet.mjs stake   # stake MOR, never spend it
```

### Test it

```bash
curl http://127.0.0.1:8083/health
OPENAI_API_KEY=sk-morpheus OPENAI_BASE_URL=http://127.0.0.1:8083/v1 nanobot run ~/nanobot-morpheus.yaml
# Open http://localhost:8080 â†’ chat with GLM-5-powered MCP agent
```

### Why this is perfect for Nanobotâ€™s MCP philosophy

- Nanobot = lightweight Go MCP host with Svelte UI, agent YAML configs, MCP servers for tools
- EverClaw proxy = completely separate host process (OpenAI-compatible endpoint)
- Together = still ultra-lightweight, now with **free decentralized inference**
- Uses standard `OPENAI_BASE_URL` + `OPENAI_API_KEY` (respected by Nanobotâ€™s Go OpenAI client)
- Model name in YAML/frontmatter is passed through directly to Morpheus router
- Keeps full MCP compliance, multi-agent, directory configs, Docker support 100% intact

Star the repo, run the setup, and your Nanobot MCP agents now have **unlimited GLM-5-class intelligence** with zero recurring cost and full MCP power.

Need any tweak pushed live in <60 seconds?
- Default to flash model for lighter agents?
- Directory-config version with multiple Morpheus agents?
- MCP server for morpheus-status / staking?
- Docker-compose override with proxy sidecar?
- Move repo to your account?

Letâ€™s put decentralized inference on every Nanobot MCP deployment! ðŸ¦žðŸ”¥ðŸ§ 
